{
  "https://www.youtube.com/watch?v=Nnwa_45AQbs": "{\n  \"webpage_url\": \"https://www.youtube.com/watch?v=Nnwa_45AQbs\",\n  \"title\": \"DEF CON 31 XRVillage  - Fireside Chat - Brittan Heller\",\n  \"description\": \"Explore emerging technology, hardware and experiences in the XR Village Playground. Meet and learn from technologists, futurists, and artists in the XR (VR / AR) space. Sponsored by BadVR and in collaboration with ICS Village, Red Team Village, Adversary Village and Policy Village.\",\n  \"channel_url\": \"https://www.youtube.com/channel/UC6Om9kAkl32dWlDSNlDS9Iw\",\n  \"duration\": 2955,\n  \"channel\": \"DEFCONConference\",\n  \"uploader\": \"DEFCONConference\",\n  \"upload_date\": \"20230915\"\n}\n\nThis text was transcribed using whisper model: large-v2\n\n Oh, I guess it is.  I guess it is.  There we go.  Awesome.  Because as we were just sitting here joking, we're like, this is absolutely the slot to  see who made it to bed early and who has already, ooh, and now I'm everywhere.  This is awesome.  We called it the hangover slot.  So welcome.  Yeah.  And of course we only have one microphone.  So the good news is that works better.  So welcome, everyone.  And we're going to take a little bit of a different approach, I guess, than just presentation  slash panel, because one of the issues with this is it's an evolving space when we're  talking about the data and the privacy and all the issues with, I mean, if you ask anyone  to define, as Britton's going to talk about, like some of the aspects of the data privacy  in XR.  I mean, we don't even have a common definition for some of the technology.  So then how are you going to define what it's doing?  So I'm Liz, and this is Britton.  And we're going to talk, first, Britton's going to lay a great foundation and give us  a framework from which to have a discussion.  And then the idea is we'll talk amongst ourselves.  And then once y'all have had an opportunity to wake up, have, you know, get the brain  cells firing, then we thought we would open it up to a conversation and questions from  the group.  So bear with us.  And I'm going to let a much smarter person than I introduce the topic and explain a little  bit of where some of the challenges lie.  Question one.  Can you hear me?  Awesome.  Question two.  Who's heard of Lego Universe?  All right.  I have a salty story for your Sunday morning about Lego Universe.  Lego Universe was a highly anticipated massive multiplayer online game like Roblox, Minecraft  that ended up being shuttered.  The experience looked great, and the company was really looking to allow users to build  their own type of content and their own environments.  But given how utterly trusted Lego was as a brand, they committed to whitelisting content  before it went live.  So they were going to preview everything to make sure it was safe for you and your kids.  The only problem was that there is a term in content moderation that Lego did not think  about, and it's called time to penis.  Time to penis literally means the duration between a person entering a virtual world  and adult content appearing in that virtual space.  And with all puns intended for Lego Universe, the time to penis was not very long.  Lego needed, amongst other things, to figure out an automated, scalable, economical way  to prevent teenagers and kids from drawing penises in the virtual world, and they couldn't  do it.  It became a wide scale problem, and it actually ended up killing the project.  This is not an unusual story.  Many companies find that their expectations about governing and moderating user behavior  and user data flows and spatial computing are turned on their head.  My name is Britton Heller, and I like to describe myself as the lawyer of the black mirror.  I am a visiting scholar at the virtual human interaction lab at Stanford University, and  I teach international law and AI policy, and I've been focusing on immersive environments  for about seven years.  There are three things that I would like you to take away today.  One, immersive media has a different impact on our bodies and our minds, and this is what  differentiates it from flat screen social media.  Existing privacy law does not accommodate for this.  Two, the different technical stacks between flat screen social media and immersive content  mean that we have different layers of enforcement when you're looking at behavioral tracking  and modification in XR.  And three, AI will not save us, and that one is worth repeating twice.  AI will not save us.  Overall, the systems that we create in 3D environments are fundamentally different from  2D environments because of the technical stack, the potential impact on users, and the uninformed  state of the law.  This leads us to three questions.  What kinds of data flows do you get from XR environments?  What does this mean for business models and the law, and why does this matter to users?  Imagine that you and I are playing a race car-oriented game in XR, and I see this red  McLaren, and I really, really like this car.  I was going to say a Tesla, but the one that they're hacking over there, I don't know if  it's actually going to be able to drive after this week, so I like the McLaren.  You can like the Tesla.  When I see this car, my body starts to physiologically react to my pleasure, so my heart rate goes  up a little bit, my skin gets a little more moist, my pupils dilate.  I really like this car.  Later on, as we're walking around the conference and we go into a different experience, I see  a similar red car drive by, driven by someone who looks more than a little bit like me.  I go to a virtual sports bar, and someone who looks exactly like my crush says, isn't  it a great time to buy a new car right now?  I go to my email, and I start getting targeted ads from Geico about why I should re-up my  car insurance.  This sounds like science fiction, but the framework for it actually already exists.  Companies have been putting advertisements in XR environments since 2021, I believe it  was announced, and I've written papers about how advertisements in immersive contexts are  actually experiential.  They don't look like billboards.  You actually pay for the privilege of having the car advertised to you.  I developed a term called biometric psychography to describe the potential for this harm, where  your likes, dislikes, and consumer preferences are married with your physiological reactions,  often involuntary.  This is not against the law.  This is not covered by normal privacy law, because privacy law covers your personal identifying  information.  When you and I entered into the game, I had a verifiable billing address.  I had a user account with my authentic identity.  The companies know who we are.  That's not what's at issue.  It's the inferences about our behavior and our physiology and our preferences that are  really at risk.  It's your mental privacy, not your identity, so privacy law is not the right regime to  deal with this.  It actually makes me a little bit upset, because I'm a lawyer, and I'm like, I don't like it  when new technologies are a misfit with existing law.  Because there are different psychological and neurological impacts than socializing  with people in video game worlds, I'd like to review the three characteristics that make  something immersive.  I think when you understand this, you really get that this is not flat screen media anymore.  First is presence.  Presence has been studied in communications departments for decades, and formally it's  called the illusion of non-mediation.  When you go into an XR environment, it feels real.  The first thing that we do at the virtual human interaction lab when we have somebody  visit is I have them walk the plank.  I put them in an experience where they are over a chasm and they are walking along the  plank because nothing brings the feeling of presence to you more acutely than feeling  that vertigo and feeling your heart rate rise and seeing how your physiology reacts  to the screens and the spatial audio and the impact to show you that this is not a Twitter  feed.  This is not Facebook, and therefore maybe we shouldn't be regulating it in the same  way.  Jeremy Bailenson, who is the founding director of the lab, says we should think of VR not  as a media experience, but closer to an actual experience, and the visceral pit in your stomach  you get when you have fear of heights, that's what that is.  And we do that before talking about empathy and education and all of the other benefits  of VR so that people understand its power.  Second is immersion, and that's when users really feel like they are in a virtual environment.  A virtual violin performance has to have all of the trappings of watching a performance  in an actual theater, different views from different seats, different movement flows  through aisles, spatially oriented sound, so it's the way that XR experiences use your  sensory input to stimulate the user through light, sound, sense, tactile input.  Finally there's embodiment, and this is my favorite one because it reminds me of Looney  Tunes.  Embodiment is the feeling like your avatar or your virtual body is your actual body.  And a researcher named Mel Slater took a famous social psychology experiment called the rubber  hand experiment, and he had people look down in VR and see a rubber hand placed in front  of them and then proceeded to go Looney Tunes on it, to hit it with a mallet, to stab it,  to really abuse the virtual arm.  And when you watch the recordings of these, you can find them on YouTube, people scream,  people jump, people react like it's their actual arm.  This is the power of embodiment, and it can be beneficial in medical applications if somebody  has phantom limb pain where you want to show them an arm that is not theirs and have their  brain actually associate what happens to that with their physiognomy, but it also can place  users at risk considering the violence and other user harms that we're starting to see  emerge in XR.  So if this feels so real to our bodies, what does it do to our minds?  Tom Furness, who is one of the inventors of VR headsets, says that when something happens  to you in XR, it's like it's imprinted on your brain in permanent ink.  The experiences that you have in a virtual environment are processed through your hippocampus  in the same way that you create memories like you and I sitting here having a conversation  today.  It's more like entering somebody's living room and screaming in their face than it is  reading critical words on Facebook or Twitter.  Because of how XR impacts our bodies and our minds, it's a mistake to just cut and paste  terms of service from flat screen social media and presume that they're going to do the same  thing.  That sounds really obvious, but that's what a lot of companies are doing these days.  Second, I promised we'd talk about different stack layers and moderation vectors in spatial  computing.  What that means in normal people speak is that when you do content moderation on Facebook  or Twitter, you look at two things, conduct and content.  When you move from a 2D to a 3D environment, you have to look at content, conduct, and  environment.  So the architecture of your world, like we talked about with the penises in Lego universe,  becomes a moderation factor, one that you don't have when you have a text-based social  media environment.  That means that there are actually different ways to mess with people in XR worlds.  I like to think about spam and how does spam translate into XR?  Pretend you and I are tired of my sports car, so we go to a sports bar.  And in the sports bar, I can buy Liz a beer.  I don't want Liz coming to my bar, though, or playing on my pool table, so I buy her  so many beers that it fills up the virtual pool table and she can't play the game.  Impeding a user's gameplay in an XR environment is the functional equivalent to spam in a  flat screen computing environment, but it would never be defined as such if you didn't  think about it in three dimensions.  I'm going to skip a little bit ahead because we have a lot to talk about.  Platforms are having a lot of challenges trying to create trust and safety regimes and privacy  regimes because of all of these complicating factors.  This is a new form of media with new impacts on your body and mind.  You need new rules.  A lot of people ask, why didn't Lego just create a filter to screen out all the penises?  The answer was that most social media content is moderated by automated filters in an ex-ante  fashion, so before it ever hits a flat feed.  Facebook takes 97% of hate speech off before you ever see it.  And the way that they do this is by using classifiers.  So it's basically a huge AI profanity filter.  This is not possible in the XR context yet.  We don't have scalable, affordable, reliable ways to create classifiers for 3D environments.  We don't have the data sets yet to use.  Computer vision is the way that I think we're going to be able to do this, but think about  all the different ways you can describe a cat.  We don't have publicly available data sets yet to try to audit what a cat would actually  be in its many forms and colors and shapes and sizes.  Currently when companies do content moderation, one, they just don't do it for XR.  Two, they use a voice to text transcription and then run it through social media filters,  which does not catch context or behavioral or environmental factors.  Or three, they try to use realtime behavioral tracking, which is just not scalable and puts  the burden on users.  When you report a violation in a platform like Horizon Worlds, it takes one to two minutes  of content and gives it to the company.  That normally is not enough to determine the context of what's actually happening,  and if you watch videos of people who are reporting abuse happening, you often can't  tell who is saying what to whom and what's happening, especially in 60 seconds.  Reporting is done differently as well on every different platform, so that's like if we went  to Vegas and you call 9-1-1, but then if you went to Henderson and you had to send  them a fax, and then if you went to Reno and you had to show up in person.  When you use public safety services, if you had to call for help in a different way in  every different city you went to, you would never use 9-1-1, and I think companies are  finding that it's the same way if you don't have standardized safety protocols across  different XR platforms and XR worlds.  Generative AI looks promising as a tool to help, but it does not yet have the discretion  or the contextual understanding necessary to help with the nuance that will be required  for helping to modify or govern user behavior.  Realistically, AI-based solutions are several years away from having sufficient sophistication,  enough data, and adequate training to work in spatial environments.  So overall, the differences between social media and XR are stark, and they emerge from  the interactive features of immersive experiences and the neurological impacts that this has  on users, and this has resulted in violations of what I call the right to mental privacy.  Other scholars in Chile have called this neuro rights.  Brain computer interface scholars are talking about a battle for your brain.  It's a new book that came out discussing that, and they're all part and parcel of the same  thing where the sanctity of the content of your thoughts and the inferences that can  be made about your health and your sexual preferences, your political preferences, your  gender, your age, all of these things we can determine from data flows really shouldn't  be used to send you advertisements.  If so, I think it's actually a failure of the imagination.  I think that's a good way to turn it to words.  One of the things that really strikes my mind is it's not just the gaming experience.  This is moving beyond so many different aspects and potential.  So as Britton was talking about, moving into the healthcare environment, that your healthcare  will be delivered potentially through this data that is being collected from your physical  reactions.  So the questions start to become, is this data painting a correct picture?  Is this the full picture?  Is it picking up on all the nuances, and who's getting access to that?  And there's a big difference between what I may post on social media at 2 a.m. when  I am fired up about something, and what I would actually say or do, where you think  of a lot of the, as we saw during remote environments where people were having job interviews over  Zoom or other conference calling, and a lot of it dealt with, there were companies, and  some states have started taking a harder look at this, where they were creating biases that  if you didn't make the proper amount of eye contact, or if it looked like you were distracted  during because the sensors on the screen weren't picking this up properly, that suddenly you  were getting penalized during your job interview because you weren't meeting this set standard.  And so now what happens when we take that, and all of those AI data sets that are expecting  certain behaviors, they're not expecting children to start drawing inappropriate things, which  anyone who's been in an elementary school, you should have expected it, and you should  have expected fart jokes, you should have expected lots of poop, and all those different  things, but what happens when we're trying to create protections, and we're trying to  do all this with a bias towards like we're already starting so far behind, and now we're  taking all these other little cues of how does the VR head to head, like how does that  experience know that it wasn't because a spider crawled across your foot, or someone else  in the room thought it would be hilarious to tap you on the shoulder as you were playing  the game, or I'm not saying this has ever happened, I've definitely never done this to  someone playing, but what happens if you move furniture while they're in the middle of and  suddenly they're tripping over something because it was really freaking funny and it just serves  them right, but suddenly that reaction is getting captured and how do you know that I haven't  logged into Britton's account? That I'm not playing as her, and should I be so lucky to  have all the sensors pick up and be like, hmm, you have the same facial, you know, structure,  you have all this, and I'm like, yes I do. Yes, that's right. Or you're just off today, or  heavens forbid, I think of all the times I've tried to log in to my iPhone and it's like  facial recognition, not, I'm like, wait, what? No, oh my god, are you telling me I didn't get  enough sleep last night and suddenly, or, oh, god, is my resting bitch face back? Like, oh, I  didn't mean to do that. So how do regulators and how do you work all those different aspects  in? Because we're already playing catch up.  One of the things that I like to do when I talk to regulators is tell them the type and quality  of information you can get about a person from these data flows, and I realize I didn't mention  that before. There was a study that came out last week, so this is the absolute newest  information from UC Berkeley, and the study said that a machine learning model was able to  accurately predict the height, weight, age, marital status of a user in XR by the way they moved.  So I know earlier I said that it doesn't go into the realm of personal identifying  information, but initial studies are starting to show that you can actually tell somebody's foot  size by the way they move. The lab at Berkeley and the lab at Stanford have had some similar  studies, and Berkeley came out with a study that said with, I think it was 100 seconds of  recorded data, you could uniquely identify a person in XR with 94% accuracy. From a data set  of 50,000 people. Stanford did this earlier, but the data set was, you know, a couple thousand  people. It wasn't statistically significant. 50,000 people. That might be you in the Taylor  Swift crowd. By the way that you tilt your head and point can be uniquely identified in an  immersive environment. The type of medical and sort of human rights oriented information that I'm  concerned about really is tied into eye tracking. So the type of information you can get by  somebody's pupil reaction, you can tell who somebody is sexually attracted to. You can tell  whether or not somebody is likely to be telling the truth. And you can also tell whether or not  somebody shows preclinical signs of certain medical ailments like schizophrenia, ADHD,  Parkinson's and Huntington's. These are preclinical signs. These are things you may not know  about yourself yet. Your doctor may not know about you yet. And a company, if they were to  parse the data set, would have access to that information. This is what I mean by the right  to mental privacy. It's not just the content of your thoughts. It's the implications that can  be made about who you uniquely are, your medical conditions and the type of behaviors that  you may choose to engage in or not.  One of the things we're seeing is the states are starting to try to play catch up. Because  keeping in mind, data brokers are taking this information and creating these pictures and  they're selling them. And the regulations really are not keeping up with who should have  access to this and how do we protect it. And it's also what happens if it gets wrong? That  one out of 50,000, it's wrong. And but yet this is now flagged in a file and it's, you know, as a  child you're told this will go down in your permanent record. Well how do we fix that  permanent record when it's our biometric and it's our mental information that, oh wait, you  know, objection your honor. I was just having a bad day and that wasn't me. Or the, you know,  it that was someone else and that piece of information. So it's not the entire picture that  gets off. It's just one piece. How do you fix that? I don't know. I don't know. I don't know.  I don't have a good answer for that. I wish I did. Because right now these type of data sets  aren't auditable by the public. In certain jurisdictions people are proposing a right to have  all the user information about you available. But these are profiles made to sell you  something. It's not an essential part of your user account and things. Another thing I tell  regulators is that you can't outsmart the machine. Because a lot of them think that, you know,  I'm somebody advertising something to me, that's obvious. That's not, I'm not going to fall for  that. In XR environments you can actually switch out elements in the environment at blink  rate. It's been done in experimental context but there's nothing saying that you could have  somebody with a can of soda and switching out Coke and Pepsi and tracking somebody's more  positive reaction to one versus the other. It's kind of like that social psychology experiment  where you're told to count the number of basketball passes and so you miss the gorilla walking  in the background. It's a similar type of a dynamic where people think they're smarter than  the system but the system works faster than you can articulate your preferences or requests  cognitively. And that's a really kind of funky, scary thing. If you watch sports and Liz is  here and I'm back in California, Liz and I will actually see different ads behind the players.  Augmented advertising is already a thing. And it's based on your zip code. And somebody in the  stadium will see even different ads than Liz or I would see on different network channels. They  sell this space. So advertisement is in immersive context is more about targeting and  personalization than it is in an offline context. And that makes sense but the capacity and the  ability to really target it to people makes XR the most persuasive medium that we've developed so  far. One of the things that we saw and we keep going back to is the data sets. What is it  getting based on? Where are those data sets getting built from that are creating some of these  models? And being able to audit it. And being able to audit and get to your information and  make sure like hey, is this the picture that I see? And basing it on a bias towards well,  English speaking U.S. based things when you're talking about an immersive experience that is  intended to open up the world. Like the idea is that you are playing and you are experiencing  things with people in any different physical location. So now how do you build the rules and  even train that it's getting it right when not everyone's going to look the same, react the  same and different cultures have built in different you know, mannerisms. And as we saw with a  lot of the you know, again even just trying to unlock your iPhone, heaven forbid you're a  woman of color. Because guess what? All of the modeling that was based on was not you. Not even a  little bit. And so they had to go back and say oops, our bad. Well now what do we do when  we're talking about collecting all this data? It's like on the reactions of you know, we talk  about a lot in the medical world a lot of the early training was done and it wasn't done based  on women's bodies. And it wasn't based on. So it's like oh, well you're going to react to this  medication in the following manners or these are the following symptoms. It's like but no? I  am not. So now you've got the VR and it's assuming heaven forbid I don't want to be a red  head middle aged woman in my like I want to be a kick ass like ninja dragon whatever the hell  I want. So how are we going to start building that and work on regulators when you've got  because again heaven forbid. Heaven forbid that Australia, the U.S., New Zealand like insert  country get the upper hand in being the regulator who sets the rules for this. Because oh no we  can't let Canada do that. I mean they're just nice maple syrup. So how do you broach those  conversations when you're going all over the world and talking to them and saying no no we  need to let this country take the lead or play nice with them and not share the data?  If you'll notice I give Britton all the really hard questions because this is what she gets for  being so smart and being a professor, shaping young minds that are creating this future. So  sorry. But those aren't young minds those are like crusty old minds that's why I'm like  hmm about this. A lot of people in Europe argue that GDPR should cover it. They argue that  Article 1 of GDPR should cover all of the privacy protections for XR and then we're done. I  take issue with that. Because these are new types of data flows and they're not really  anticipated by the regulatory structure. And they're more akin to medical data to me than  they are personal identifying information. The future on privacy forum just did a really  optimistic study where they determined that biometric laws in the U.S. are covered by  state based law. And if you have a business that creates XR products you know this because  there are certain states you're really really worried about selling in and certain states you  may choose to not release your product in. And those may be Illinois, Washington or Texas.  There are other states that have privacy laws not biometric laws but you think about sometimes  I talk to regulators and I ask them if they've tried smellivision. And they look at me like  I'm nuts. And I tell them that in the lab and in commercial context now you can actually go  before a campfire in XR and you don't just hear the campfire and you don't just see the campfire  you can actually smell the charcoal and in the lab we put a little toasted marshmallow in there  as well just for fun. You can go into a garden and smell flowers. These type of inputs are  really not anticipated by law. Also haptic there was a company that recently released a breath  gauge for a meditation app. And my question for them is is that biometric data? It could be  body based data based on some of the states. So it would be regulated as such. In other  states it wouldn't be. I think the way that I make regulators understand this is I try to  actually bring them into the developer's mindset and bring them into the chair of somebody who  has to make a decision about selling and producing these products. And doing so in an  environment that is ambiguous at best. Some countries are better about it than others.  Australia has an e-safety commissioner's office so they're very very motivated. New Zealand is  very concerned about public safety in the online context. France has had a national metaverse  strategy. China and Colombia and one other country they have started a strategy called  trials in the metaverse. Don't do that by the way. That's not a good idea. So countries are  starting to engage in little ways. I think Bahamas had an embassy in the metaverse. I think  the way to actually get to these countries is to talk to them more about their economies  and corporate environments and that should get them more motivated rather than talking  about digital diplomacy and kind of a new sovereign space or other type of nonsense things.  When you think about how even in the U.S. a lot of our laws that touch on things that we deal  with day to day, the DMCA, CFA, a lot of them came because members of congress for example  were afraid that their blockbuster rental history was going to be published. Oh yeah. Or they saw  a movie and they said you know what? War games. Huh. Huh. So I love that idea of putting it in  that personal context because unfortunately that does ground it for them and show them because  so often we spend time chasing the technology like oh we have to react to this. When it's like  no, let's look at what it's actually doing. What is it doing at its core? What is the actual  concern? And then how do we build from there? So I love that you're getting them that hands on  but how do y'all scale that kind of advocacy or how do we as a community of users who are  concerned about or builders, you know, how do we take that idea and that approach and scale it  beyond you know and bring it down to that local level or that regional level of hey, do this,  see this, experience this and know that this is going to impact what you're doing. I think you  don't use law. I want to call out someone in the front row. Caroline Sanders whose research  about how to use effective design to create healthy online communities and environments is  awesome. Yay. So looking at design based principles, looking at social psychology, looking at  how we create cultures. Some of my very boring academic work focuses on the theory of code as  law and how that has ignored norms. And norms are the way that we create culture and we self  regulate our own online environments. And try not to place the full burden on users but  distribute it collectively amongst a whole community. That, it seems like a cop out answer  because I'm a lawyer. But this isn't a legal problem I think. It's more of a social problem and  an issue of science not catching up to politics. Well and I think that also putting it on the  users overlooks the builders, the makers. Like if we are intentional from the design aspect  and when I say we, I mean the people at the companies who are building it from product to  engineering to all that understanding and seeing that big picture and knowing when to call  like oh this is going to impact this. Let's be intentional. Let's bring them into the  conversation. So as a DEF CON community I think that's one of the easiest ways to  start. I was like start the change. And I think we can also be really intentional about how  we design these products and who we are accommodating when we design them. How many people  here are familiar with simulator sickness? Probably a bunch. If you're not, it's if you've  gone into a VR world and you feel nauseous and sick to your stomach and really disoriented,  that's simulator sickness. In the earlier iterations of headsets, more women than men reported  simulator sickness and people started to write about this and some said it was just my delicate  female constitution and the fact that I may produce babies one day or the fact that I don't  play first person shooter games. It's all bullshit. What actually was happening was that the  headset was designed for an average male form. Between 5'10 and 6' tall. There is one  measurement that is essential to preventing you getting simulator sickness. It's the  interpupillary distance. The distance between your eyes. If you go to the optometrist they  measure this to put you in a pair of prescription glasses. The interpupillary distance on  early VR headsets was not adjustable. So it was akin to putting 51% of the population in the  wrong pair of glasses. No wonder women were reporting sickness and discomfort. And there  were studies following that by Jessica Outlaw that actually showed people what women  weren't returning to VR environments because they felt uncomfortable. There were similar  studies that came out. One from the MIT media lab where there was a researcher doing her  master's thesis work in Nairobi. She took Oculus Go to try to put it on Kenyan entrepreneurs and  she found that with the women, 50% of the time the straps snapped when she tried to fit it  over their hair. 50% of the time. She ended up deviating from her master's thesis to design an  adjustable fucking strap. Easy fix if you think about the wide beautiful variety of people who  you want to be your customers. It's smart to think about the wide diversity of body shapes and  types. Disabled populations are some of the earliest adopters of XR hardware. And they are  the last people that many companies think about when they design these products and services.  Oculus didn't change their vantage point to make it adjustable whether or not I was standing or  sitting until update 31. And it came on the heels of a Scientific American article that showed  that somebody had to modify an Xbox controller to use a rock climbing game. They were a user  with muscular dystrophy and so they really wanted to know what rock climbing felt like. And  they had to modify it themselves because the game wouldn't accommodate them. This is  something that we can do as a community. So when you're doing your prototype testing or your  platform building or your experiential design, think about all of the people you want to  in your world. And then go bigger.  Yeah. And I think bringing it back to what Britton was talking about before is like that full  stack of an understanding it's not a level playing field. And by that I mean it's not a flat  screen. And so as you're building it and as you're designing it, taking all of those different  aspects and also incorporating it into the conversations on the biometric data. Like what  exactly? So we're not using those same definitions. It's not the same game. And being, again,  intentional in how you're building, how you're designing, but also how you're approaching it.  And be it with the conversations you're having with, you know, regulars as you're watching  that, I think really just kind of bringing it all. This is not, you know, this is an immersive  environment. It is the entire all encompassing environment.  And there's some best practices that you can follow as developers or even as security experts.  A best practice for XR is to actually keep the data on device. That means that you have to get a  warrant to access the data. The problem with that is it really impedes your processing speed.  It's very, very expensive. It kind of drains your battery. So the other thing you can do is  when you're looking to third party cloud providers, you can be really deliberate about asking  them about their data retention, storage, disposal, all of their data policies. Because what  your third party data provider does is basically what you do. And being a savvy customer. So  pushing back if there aren't privacy protective services like you feel meets the threshold  that you want to protect your users.  And that's excellent. Because the change starts with calling it out. And bringing it to the  attention of and paying attention. Because you also think of all the different users and  whether they're going to understand, you know, a 12 year old child is not going to care about all  their different things. But it's like all they know is there's a lag time. And they're annoyed.  It's like no, let's make sure that we are paying attention, that we are bringing it in and  that we are reporting it, raising it. It's kind of the name and shame. Hey, this is a great  game that you've created for children or geared towards this market. Now you're creating their  permanent record for their school. For that, you know, the equivalent of that example that's  going to follow them forever and is going to be based on AI modeling and data points that are  not necessarily accurate. I mean they're going to be based on if we're looking at the early  sets, the 5, 11, 5, 10 male, you know, interactions and all of that different things. So what is  like if you had to think of like one kind of way to bring it home of how you're dreaming of  electric sheep, what are some good takeaways other than I mean I love what you were saying  with the proactive steps. What's a good nugget? What's the quote?  It's the hangover spot on Sunday morning. I don't know if I have a quote.  Someone asked what is inspiring in this space which is a great question to end on because it's  all doom and gloom when I go to the dinner party. XR is magic. It's magic. It makes it  can it can make you feel like you are in a different body. You start to embody the  characteristics of your avatar in a social environment within two minutes. So if I make  myself an 8 foot tall dragon, I'm going to dominate that space in two minutes. It takes two  and a half minutes for me to put you in my lab context and give you a third arm and have you  pop three balloons at the same time. Neuroplasticity is magic. XR is a means to access  your neuroplasticity. Your imagination is really the limit and we've never had a technology  like this before. So let's not fuck it up. That's my quote. I like that. No, thank you so much  and thank you to the XR village for bringing this topic and inviting us. Please go check it  out if you haven't already. I'm Lawyer Liz on Twitter. Please feel free to engage. I'm  Britton Heller. You can find me as Britton Heller on Twitter or on LinkedIn. Thank you so much.  Thank you.",
  "https://www.youtube.com/watch?v=O1xhxAie7ac": "{\n  \"webpage_url\": \"https://www.youtube.com/watch?v=O1xhxAie7ac\",\n  \"title\": \"DEF CON 31 XR Village - The History of XR  From Fiction to Reality - Starr Brown, Bob Gourley\",\n  \"description\": \"In this discussion XR village Executive Director Starr Brown interviews Bob Gourley, whose site OODAloop.com has been tracking XR topics since 2003. Bob has leveraged his experience as a cybersecurity professional and an enterprise CTO to produce research and reporting on XR that points to a future of incredible potential, if we can mitigate the new threats this emerging technology brings with it. Starr Brown is a security professional with a knack for using collaboration and innovation to meet both compliance and security needs and was early in identifying the unique security and risk mitigation needs of XR, making her the perfect person to extract insights from Bob in this fireside chat.\",\n  \"channel_url\": \"https://www.youtube.com/channel/UC6Om9kAkl32dWlDSNlDS9Iw\",\n  \"duration\": 945,\n  \"channel\": \"DEFCONConference\",\n  \"uploader\": \"DEFCONConference\",\n  \"upload_date\": \"20230915\"\n}\n\nThis text was transcribed using whisper model: large-v2\n\n Welcome to our talk. I am Starr Brown and I am pleased to introduce Bob Gorley of  say your company, OODA, Datalink, and I am the executive director of the XR  Village and you're joining us today to understand from fiction to reality the  history of the metaverse and XR. Thanks Starr. And Starr, first I want to say thanks for  pulling together the XR Village. For those of you who haven't done it yet  please sign up for our discord because there's going to be a lot of persistent  content in the discord including things like these slides that we'll put there.  And the XR Village has got to capture the flag going on right now and some  demos with some advanced technology that's really awesome. What we'd like to  talk about today is the past present and future of the metaverse and some  threats to the metaverse. We thought I'd start with some slides that I'll  quickly go through just to talk about my view of the threats and then Starr and I  will have a discussion on the topic. Sound about right? Sounds good. Okay let's  do it. So the topic of these slides is defending the metaverse and I wanted to  give you just an overview of what I'm trying to capture in these notes. If you  want a copy of this just send me an email or come to the discord server for  the XR Village and I'll post these there that you can download. So we are going to  talk about the metaverse and what it is and what it's not and how that relates  to XR. We're going to talk about then the key threats and some ideas on  mitigating the threats and we're going to talk a little bit about the history  of this stuff. Before going further I wanted to talk about the history of XR.  XR meaning virtual reality, extended reality, augmented reality, all together  has a long history that I think actually starts in science fiction. There was a  1935 science fiction story called Pygmalion's Glasses which outlined what  the metaverse is talked about today and what XR is talked about today. Science  fiction, a nice short story but it was really an exciting vision of what could  happen if you can put on goggles and be in a different world and that's what  we're building and experimenting with today in the XR Village. But there are  threats and that's what we wanted to get into with this presentation. I say  there's five threats but I'm going to talk about a sixth one that I learned  about today and with your feedback maybe there's more. We need to talk about the  threats so we can figure out how to mitigate the threats. The bottom line up  front, we are building through extended reality a new system, a metaverse, a new  wave of technology and it is not going to be more secure than the current waves  of technology. The three waves as we see them, the first wave was the internet  itself connecting computers and servers together in an interoperable way. The  second wave of internet technologies is the mobile revolution that based upon  cell technologies that included mobile devices and all the applications that  came with that. The third wave that is now being built is this XR wave. It's a  wave towards the metaverse. It really is the future. The first two waves are still  going to be there for a long time but the third wave, the XR wave, is what is  being built right now and is why this village is so hopping over here. Next  please. So what is the metaverse? You hear metaverse frequently. My view is don't  lock into anyone's specific definition. It's far too early for that but what is  being built is a massive scalable interoperable network of networks and  persistent virtual reality and augmented reality worlds. That metaverse is being  built but it's not here yet. We have hints of what's coming and I'll show you  a few on the next slide. Next please. So these are some of the hints that we're  seeing. I don't think of any of these as the metaverse. They're just indicators of  what's coming when it's all linked together in a scalable persistent way.  You have all seen the business collaboration tools and the education  collaboration tools. I think you've probably all seen the games that are out  there that can be accessed through any computer or mobile devices through  extended reality, augmented reality, virtual reality. You don't have to have  the goggles but the goggles help a lot too. You have also seen the other tools  that are being the games like Roblox, Fortnite and others. The digital twins in  the business world you've seen. These are all hints of what's coming but it's not  the metaverse yet. Right. Next please. The metaverse is coming however and that  metaverse of tomorrow is going to impact so many industries as you can see over  in the XR village and get hints of what is coming including just about anyone  who needs to work with data for a business or professional reason but also  anyone who likes entertainment in a nice 3D interactive way. It is going to be a  trillion dollar industry. Next please. I'd like to jump in and share that we have  a live working model of a digital twin scenario with in collaboration with ICS  Village. It's off the rails and you can interact and explore practical uses of  this novel technology. Cool. You remind me of another thing that I think we should  have told these folks about and that is Bad VR. Bad VR is awesome and first of  all it's a great name for a company but Bad stands for bring all your data to VR  and there's a demo in your village. There is all new ways to visualize real time  with spatial packet capture and directly into the goggles so that in  your living world you can see Wi-Fi signals, other cellular signal, RF  signal and so on. Awesome. The way the business analysts look at this trillion  dollar industry is shown here. Next please. Now I want to talk about the  threats and when I built this I was thinking of five major threats that are  coming in the metaverse that are not being dealt with yet in my opinion. Now  running a village I want to be able to take input from everybody in the village  to see if this covers all the threats or not. I did hear of a sixth one today I  want to tell you about. Brief review of these threats being built I mean that  are impacting the metaverse. There's new privacy threats we all need to worry  about. If you have a device on your head and that is collecting data and moving  that data to the cloud there are privacy issues that we need to sort out today.  There's an issue of languor. What happens if you put on devices and it just pulls  you into this world in ways that make you want to detach from the real world  and if too much of that happens in society what's going to happen to all of  us? And that relates directly to the third one the onophrenia which is an  inability over time to distinguish what is real and what is not real. A very  serious threat. Absolutely. There's new fraud vectors. If someone can know so  much about you including what you really desire it's going to make it easier for  them to commit fraud against you. And then I list the last one here of  unleashing demons. The worst of human spirit may be tapped into and brought  out in ways that are not good. Now I wanted to talk about the sixth threat  star. It's one that I it really didn't dawn on me this threat until  conversations this morning about the future of all this technology and it's  the threat to us humans and our livelihoods. Now this relates to far more  than just XR. This is XR plus artificial intelligence plus overall better  automation and technologies and we see what's going on right now in Hollywood  to screenwriters and actors and actresses and this is a serious threat  to them but it's also a threat to journalists. It's a threat to truck  drivers. It's a threat to those who work in agriculture in the mining industry.  This is another threat we need to think about. The future of XR is that a threat  to us humans which leads to the need for frameworks to solve these threats  and I'll just rapidly push through these frameworks. I believe personally we need  to think through what does everybody need to do to mitigate these threats and  I'm looking for open ideas here so please get a copy of these slides off  our server. Read through this. Give me your feedback. How do we address these  threats? I've put in suggestions for individuals, communities, companies,  governments and academia and we don't need to go through all of this right now.  It's my suggestions. Things for people, us to do. We need to keep learning for  go ahead and hit the next for communities. The communities like DEF CON  community. What do we want out of this new world of extended reality and  virtual reality and enhanced and augmented reality? We need to figure that  out for companies. Companies have a role to play here too. They really need to be  building in security that meets our requirements. So I interject here.  The thing with building, you hit a keynote for me and the point is that we  need to start thinking about building products, software, hardware and policy  better by design from the beginning so that we avoid problems in the future.  Yeah. There's a role for governments. I don't want the government in our VR  but they're going to be there and also I think there's a role in our government  in pushing back against hostile nation states that are going to be in our  extended reality. So we need to decide what we want our government to do in  this world. Yeah absolutely. The entire purpose of XR Village is to drive better  public policy and it's not no one likes being told what to do but the reality is  that for society to function we all need some set of regulations. A framework is a  good place to start but we need to get collectively informed, educated, have a  little fun but then go make the rules better. Make the rules we want to see  because we're in a position now with emerging policies and other legal  representations in Washington and beyond that these conversations are happening.  So now is the time to scream to make sure that we can inform better public  policy for tomorrow. Yeah what a great goal and it's a great opportunity to  engage with this village which gets to this recommendation list here. Join our  discord server, come meet with Star over in the village and get engaged and that  Star is kind of the end of the graphics really if you wanted to transition to  the discussion portion of our time together here. Yeah I think I think the  time is now. I don't know how to make that go away so we're gonna live with it  but the point is is how many people in this room knew what extended reality was  before sitting down? You knew the umbrella term. That's wonderful because  every single person with the exception of one has said I have no idea. I've  talked to thousands of people so far this weekend and in a room of computer  people or security people I might expect that you'd have more more understanding  of at least what the umbrella encompasses so that we can understand  the vulnerabilities that lie ahead to help. You know and Star I think this is  one it underscores for me that you are a pioneer and the other village founders  and I'm very glad this is starting. It was here last year in a small way but  now you've really grown and I think next year you're probably gonna be connected  to every other village it seems like. It seems like it and to be honest a lot of  our current setup came because it takes a village and that is the theme for the  year and we wouldn't be here without having been accepted as an adjunct to  other appropriate areas from biohacking to car hacking to ICS and beyond policy  of course and this year we're standing alone and we have pretty pretty solid  interest in the topic so I'm really here to ask the community to contribute to  think about how how the metaverse and how living outside of what you can touch  can be affected and think about the naughty things. Think about the way we  can tell people what we need to fix today. Right I think that's exactly what  I think needs to happen too. If we want our technology to serve humans and make  the universe better we need to figure out what these threats really are and  then how to mitigate them and I love the way that you said we're informing policy  because we have direct connections now into the policy world. We are we're making great  leaps ahead in some regards but we are still talking to each other you know we  need to get to a position where we can include and educate and I think we're  really at the right time to do so. You know another thing I noticed about your  village is there's a lot of young people there. It is it is this morning I think  everybody was maybe a little hungover and that's you know that's awesome but  we have a lot of children this morning in this equipment and and really  becoming some of the best demonstrators of I have you know a seven-year-old  grabbing Wi-Fi bars out of the sky and giving bad VRs demo. They latch on like  this. They adopt like this. This is why we need to think about this. I work largely  in the education space and I've encountered many teachers who are  actively using AR in the classroom. VR in the classroom. They've been doing it  for five years. They have zero concept who is looking backwards at their  children in said classroom. They didn't even know it was a thing. They didn't  know that data was stored. They didn't know that data was perhaps viewable in  ways that they might not expect. So things beyond there's temperature  sensing technology and I'll just leave that there. So it's it's a pretty  interesting field to put on glasses and see something that we're not all  realizing is being seen. And can I put you on the spot and ask you for a real  definition of XR because I'm not sure I'm using that in the proper way and how  do you define XR? I tend to think of XR as being an umbrella term. So for me  it's less about the metaverse and I think working through you it makes me  really think beyond hardware. So I think it's XR encompasses AR and VR along  with the associated virtual worlds. Wonderful. That's great. Well I really  think this village is contributing already and I'm wondering if we can  entice more people into the Discord server if we can persist this dialogue  between DEF CONs. Absolutely. Absolutely. So join us on Discord, XR Village.  You'll find our information on Twitter and on the slides and not those slides  though. So we'll come over. We're down yonder. We'd love to have you come play  and then come see the demos and check us out. Cool. Thank you.",
  "https://www.youtube.com/watch?v=Gc56q16RAGg": "{\n  \"webpage_url\": \"https://www.youtube.com/watch?v=Gc56q16RAGg\",\n  \"title\": \"DEF CON 31 XR Village - Push All the Buttons Digital Twinning w Idaho National Labs - Kolton Heaps\",\n  \"description\": \"Extended Reality (XR) is an umbrella term that involves virtual reality (VR), augmented reality (AR), and mixed reality (MR) capabilities. VR describes an environment in which a user\\u2019s physical environment is completely replaced with a virtual one. This allows the user to view any digitally created content but separates the user from interacting with the physical world. On the other hand, AR enhances the user\\u2019s physical environment with virtual overlays but offers little interaction with digital content. Lastly, MR is a blend of the physical and digital worlds, unlocking natural and intuitive 3D human, computer, and environmental interactions. Using these technologies, analysts, operators, and stakeholders will be able to interpret radio frequency data effectively and efficiently.\\u202f\\n\\nThe Idaho National Laboratory is integrating the next-generation XR capabilities into the various projects that support Nuclear, Integrated Energy and National and Homeland Security missions. They use a suite of tools for the visualization of capabilities to capture and analyze digital twins. Digital Engineering delivers semi-autonomous design, autonomous operation, and real-time anomaly detection as well as integrates threads of data, visualizations, AI/ML, and physics models into a cohesive digital twin.\\n\\nThe primary benefit of incorporating XR with signal analysis is to allow for simple interpretation and representation of complex data. Current techniques or trends rely on certain subject matter experts to collect, examine, and report anomalous data manually. By allowing the operator to spatially view the captured data, the process of identifying and plotting data is anticipated to be greatly simplified. Data and anomalies will become engaging, allowing the operator to easily identify unknown signals in real-time or near real-time.\",\n  \"channel_url\": \"https://www.youtube.com/channel/UC6Om9kAkl32dWlDSNlDS9Iw\",\n  \"duration\": 1922,\n  \"channel\": \"DEFCONConference\",\n  \"uploader\": \"DEFCONConference\",\n  \"upload_date\": \"20230915\"\n}\n\nThis text was transcribed using whisper model: large-v2\n\n So, we were asked by the XR Village to come and just give a little talk about the work that we've been doing at the Idaho National Lab, using some XR equipment.  So, behind the scenes of our daily lives, there's critical infrastructure that's supplying us.  You know, all of the necessary services, you know, water treatment, our electricity, and all of that infrastructure needs to be secured.  But as, you know, working in the security kind of aspect, we know that these systems definitely aren't as secure as we'd like them to be.  And so, part of the work that we've been doing at the National Lab, in partnership with the Seller Group, is to work on securing those systems  and helping, you know, smaller cities to know that their systems may not be secure, that there are vulnerabilities.  And we've been using XR as a component of that outreach.  So, my name is Colton Heaps, and I've been working with the Idaho National Laboratory for about two years.  It's been really exciting to be able to work with, in a laboratory where they have such unique infrastructure.  So, for those of you who don't know, the Idaho National Lab is a research laboratory that's focused on nuclear research.  And so, we have, you know, a couple of different reactors out on our campus.  And with that, you know, microgrids and a lot of different infrastructure that enables us some unique cybersecurity research.  And so, I will kind of start out by describing the group that I work with, with digital engineering,  and some terms that I'll be sharing, and introduce the Seller Group,  and then get into how we've been using mixed reality and virtual reality for that work.  So, what is digital engineering?  Digital engineering focuses on supplying data to researchers, regardless of the platform that they're performing research on.  So, in, you know, times past, we had researchers passing around documents.  They'd have sacred word documents that had all the information for the projects.  That is not a good way to do engineering.  So, we're trying to centralize all of that data, no matter where it comes from,  and enable the different platforms to be able to reach out and use that data.  So, digital engineering helps to provide semi-autonomous design, autonomous operations, and real-time anomaly detection.  It integrates the threads of data, visualizations, AI and ML, and physics models into a cohesive digital twin.  And so, a digital twin is, it's a buzzword.  So, we're trying, it gets thrown around a lot in industry, but we're trying to define that at the National Lab.  And so, I like to compare it to the iPhone.  So, we all know, you know, the impact that the iPhone had on our world.  And because of the iPhone, you know, led to other smartphones, rising popularity, and that truly changed our lives.  But the iPhone itself is made up of a bunch of smaller technologies that already existed.  And those technologies themselves weren't necessarily revolutionary, but the combination of all of those made something special.  And so, that is how I see digital twins.  And it's the combination of all of these different technologies that have existed in the past,  but the combination of all of those together makes it something special.  So, what is a digital twin?  Like I mentioned, the digital twin is a merging of all of these different technologies.  The connection of integrated data, sensors, and instrumentation from physical assets.  So, taking that realtime data from the systems and feeding it back into visualizations and AI ML algorithms.  The AI and an online monitoring and advanced visualization with mixed reality and virtual reality.  So, that's kind of the INL's definition of the digital twin.  And a digital twin mirrors the physical model while using realtime bidirectional communication to perform, you know, this magic.  Enabling the user to take in data more efficiently, giving the operator more data in a way that they can understand.  And so, put that into context of the work that we do at the lab.  Like I said, nuclear research and working with these nuclear systems.  We need to be able to have safe, small modular reactors in the future.  And they need to be able to have autonomous corrections.  So, broadcasting into the future to see if these things are safe or not.  And then if they're, you know, going into a hazardous direction, then they need to be able to fix themselves.  But we've been using it with the cybersecurity group at the INL, specifically the CELR group.  So, CELR stands for Control Environment Laboratory Resource.  And so, CELR is sponsored by DHS's Cybersecurity and Infrastructure Security Agency, or CISA.  CELR is a lab-scale environment with sector-specific platforms with operational physical components.  As well as operational IT and OT environments with real controllers and human machine interfaces to operate the physical components.  And so, there are about six of these training platforms that CELR has provided to the public.  And each of these platforms have unique HMIs, PLCs, and protocols that are used.  And so, it takes the breadth of, you know, manufacturers' hardware and protocols that are used in industry.  And displays them here as a resource publicly available for, you know, international use.  So that anybody can gain more experience in securing this critical infrastructure.  So, the environment is used to train cyber incident responders and expose them.  That actual networks and malicious activities are similar to what they would see in the real world.  Support tool evaluation to determine effectiveness in operations and research mitigation measures that may counter those effects.  CELR's capabilities have matured to include six sector-specific platforms with six different controllers, HMIs, and network protocols.  This allows for diverse options to fully represent a broader range of the nation's infrastructure.  And it's definitely a valuable resource that is being provided.  And helps to supply this resource to smaller cities that may not have the funds to put towards cyber security.  And securing their infrastructure so they can take advantage of these resources.  Okay.  So, in the CELR group, while building these systems, they started to reach out to digital engineering.  Just as a way to visualize their, you know, their plans in their new workspace.  And so, they were planning to build, they had one existing skid, I believe.  And they were planning to build more in the space.  And so, they wanted to use some of our XR capabilities that we've been using across the lab to visualize these components in their facility.  And maximize the efficiency of the space.  And so, it's maybe not a very exciting way of using mixed reality.  As it's, you know, seems like everybody's using that now.  But it's very valuable.  And saves so much time and money.  And so, we were able to use some virtual reality goggles.  And take the existing CAD models as geometries.  And import them as meshes into our visualization software.  And mainly we use Unity for this.  Because of how easy they've made it to work with a variety of hardware.  And so, with those CAD models and the BIM model from their facility.  So, BIM is Building Information Management Model.  And they took both of those models and put them into one.  So, they could move each of their training test beds around the facility.  Maximize the use of the space.  And also in the design of the physical test beds.  They were able to see them to scale in front of them.  Interact with the different components.  Make sure everything was, you know, would work just how they expected.  And so, as that partnership grew with Cellar.  We started working a little bit more in public engagement.  Which kind of led into us realizing how valuable extended reality headsets were in this work.  So, with the new test bed designs.  They had, you know, an interface similar to this.  Where they were able to have a portion of the HMI in the virtual world.  And they could click buttons to toggle on different effects.  Just run through in software the iterations of how things would look.  And ultimately save time.  And so, all of these test beds.  They represent physical critical infrastructure.  So, they have the same OT, IT components that are used in, you know.  Saying, like this is just a laboratory space.  And so, to help to increase engagement.  They have different effects.  Like that smoke that you can see in the bottom left corner.  So, when the system, you know.  Maybe the HMI is vulnerable to a certain DLL injection.  Then that can cause some actual physical damage to the system.  Maybe, you know, turning off some of the fans in a lab space.  Or reversing fans in a fume hood.  Causing gas to go into the lab space.  So, working with this critical infrastructure.  Has physical effects.  So, you know.  It's not only some database or something that goes down.  But it can actually cause harm to a system and people.  So, then we worked in public engagement.  During the first sort of half a year that we were working with Cellar.  And as you can tell by the ugly carpet on the ground.  And the pictures on the slide.  We're here at DEF CON.  So, we had the opportunity to come to DEF CON.  As well as an array of different conferences around the nation.  So, on the left is a picture of a small test bed.  That they developed for use in conferences.  Because the full test beds are like 6 feet tall.  And it would just be a pain to pack those around conferences.  So, they developed this smaller version.  Which still has real IT components.  They can, you know, share that with the public.  But then in mixed reality.  And on the, let's see.  On the left.  That man is wearing a Microsoft HoloLens.  And so, that's a mixed reality system.  And what he's seeing is being shown in the middle there.  So, because we aren't constrained to any sort of space.  In virtual or mixed reality.  We can make it as representative as we want to.  And so, there's some demo that they brought.  Hello, hello.  There's small demo that they had brought.  Emulates an electrical substation.  And so, in the visualization.  We modeled out, you know, a representative substation.  And then we were also able to expand it out.  To show more of a suburb or community.  And then in the attack.  We can show how it affects the entire community losing power.  So, at the bottom left.  You have the physical skid in the environment.  The lab space that they're housed in.  So, they just have some computers with webcams.  That are monitoring these systems.  And then anybody can log in.  And see the attacks and vulnerabilities that are on these systems.  And then learn how to secure against those attacks.  And then the webcam shows.  A kind of difficult to see view of any effects that might happen.  If that critical infrastructure is.  Is attacked.  And so, in the virtual space.  Again, we modeled out their systems.  To be able to show others resources that are available.  And then we've also tried to make a more representative view.  Of each of these systems.  And so, the idea is that.  Using digital engineering and digital twins.  We're going to mix together these physical assets.  With the visualizations.  And so, as somebody is working.  You know, going through the training online.  Then that data is being relayed to a visualization.  That they can use with their team.  You know, maybe it's a school.  Or, you know, the workers.  The cyber team of that city.  And they can have a more engaging training as they go through it.  And as well as just relaying state of the system.  We merge all of that to a central data warehouse.  That we can then distribute data between the different systems.  Whether they need to do, you know, analytics on the data afterwards.  Okay.  So, some of the other projects that we have.  I apologize.  So, just to speak of some of the successes.  That we've had with this system.  So, it was developed to be remote during COVID.  Because COVID sucks.  And we wanted to still be able to help people.  And so, they were able to see the visualizations of what would happen.  Be familiar with those things.  Have an engaging training.  But then, Cellar was still able to provide that training on real hardware.  Not emulated hardware.  In a remote setting.  And so, one of the groups was a local municipality.  And they were able to remote into the environment.  And train using their lab space.  And while they were going through the trainings.  You know, learning how to secure these systems.  That they needed to work on in their own municipality.  Their facility was physically hit.  And so, since they were remotely training.  You know, not having to travel out to Idaho.  They were able to respond to that incident.  And use the training immediately.  So, that's a big win.  And so, the idea, again, is to be able to reach out.  Help as many people who maybe don't have the funding towards these.  To put towards cyber security research.  And so, using mixed reality.  We don't have to refill the tanks for these trainings.  We don't have to physically do any maintenance.  We can just run through training after training after training.  Without worrying if it's going to break.  And so, that saves us money as well.  Lowers the cost of the resource.  And then enables us to just send out these VR goggles.  Or mixed reality goggles to whoever is signed up for the service.  And they can still have that engagement.  Okay. So, what's next?  So, at the right image.  Might be a little hard to distinguish.  But that's a router.  With some signals coming off of it.  And I'm using some quotes right now.  Because it's still theoretical.  But the research is being done.  To leverage extended reality capabilities.  To visualize wireless radio frequency signals.  And the goal is to be able to see the radio frequency signals in an environment.  Identify that it might be abnormal.  And then using the geolocation support of these systems.  Be able to pinpoint the source of the signals.  And so, second research effort.  Focuses on expanding the services to train remotely with real-time visualization.  Enable cyber incident responders to actually conduct their network analysis.  Or hunt for malicious activities on their laptops.  While then being able to see the representations of the infrastructure behind them as they work.  So, it really has been...  Although these digital twins are still in early stages.  And we still have a lot that we can do to improve the use of extended reality headsets in this work.  It's already helped us enormously.  And so, just to speak a little bit about some of the other work that we've been using these headsets for in the lab space.  So, apart from cyber security.  It's been instrumental in design reviews between these different projects at the lab space.  And so, we'll have multiple engineers come in to a visualization.  And they can view the new micro-reactor that they're building in front of them to scale.  And they can walk around, make notes on the object.  Move different components.  And talk to each other while it's going on.  With data being relayed through our own lab-built multiplayer server.  And so, that has really saved the national lab millions of dollars for each project that we've been involved in.  Because they've been able to catch mistakes early on before the project was actually built physically.  And then, share just one more project that we've been working on by creating a full digital twin of a nuclear reactor.  So, it's a small reactor.  It's about five watts used for testing.  But we were able to create a digital twin.  A high-level digital twin that incorporated AI predictive analysis on the system.  And so, it would predict 15 minutes in the future using the data that it's learned off of.  Predict 15 minutes in the future if the system was going to be passing some sort of a temperature threshold.  Or threshold on the neutron flux.  And because of this, it would then warn the operator.  And take autonomous control of the system.  Bringing it back into its operating range.  And a big portion of that is being able to give the operator a view into what's being learned through the AI.  And how the digital twin is actually running.  And so, we've given them two interfaces between the mixed reality headset and a web display.  And it's truly been valuable.  And has been integral for the research that needs to be done for these smaller modular reactors.  So, I will just open it up to any questions.  If there are any.  And seems to just be a microphone right here.  And then if not, we can go and rest.  So, there's a microphone right there if you'd like to, if you had a question.  Which reactor were you modeling, out of curiosity?  Was it the ATR or no?  I'll just give you this microphone.  Which reactor were you modeling?  Was that the ATR or a different one?  So, you know a little bit about the lab.  Nice.  So, his question was, which reactor were we modeling in the digital twin?  And if it was the ATR or advanced test reactor at the Idaho National Lab.  And so, this reactor was actually a university collaboration between the Idaho National Lab and Idaho State University.  And so, it was using their AGN-201 reactor.  AGN-201.  Okay. Thank you.  I'll try this.  Okay.  You were saying that millions of dollars were saved because they were able to catch stuff early.  I was just curious.  Do you have like one example?  Like just an anecdote?  Yeah.  Great question.  Thank you.  So, his question again was, I had mentioned that we'd saved millions of dollars on construction on these projects using extended reality hardware.  And he asked if there were any examples I could share.  So, there are some glove boxes that we've been developing to be able to process some different materials.  And they were able to, you know, again, we were loading the CAD models from the engineers into extended reality environments.  And then, they were able to visualize the glove boxes within the space that they were to be built.  And they could go around, have design reviews with each other.  They noticed that ergonomics on the system was not very good for all of their operators.  And so, they had, you know, people put their arms into the virtual glove boxes.  And they found that they were going to have to build some platforms for some people to stand on to be able to use the system.  And then, that just kind of defeats the whole purpose of building this multimillion dollar system, right?  And so, they were able to lower things down so that everybody could use the system.  On the same project, they were able to move the glove box around in parts, in the parts that it was going to be built into.  And check if they were even able to fit it through the doorways of the facility.  And so, they were able to determine that there would be some collisions.  And they broke a couple of the pieces into even smaller sizes.  And then, clearances between doorways has always been, seems to always be an issue on whatever project we're on.  So, just escape routes and things, they need to have certain, you know, clearance between the doorways.  And they were able to find those and make adjustments.  And so, that's a smaller project.  Some of the other larger projects that we've been on, you know, if they were able, if they were to go into construction at all on those,  before they caught these design changes that needed to be made, then it would have cost millions and millions of dollars.  So, yeah, thank you.  Can you say a little bit about, can people hear me or not?  You might need to get closer.  Yeah, hello.  Can you say just a little bit about why virtual reality, as you've described it, is actually better than traditional, you know, on the flat monitor,  you know, which has been used for many, many years now to catch many of the same sorts of mistakes.  You know, I noticed that, actually, that your glove box example is probably a good example.  But do you have any thoughts or, you know, why is this, you know, better than the traditional, you know, flat screen CAD?  Yeah.  So, his question is, how does mixed reality differ from using just normal 2D CAD software?  And how it's been beneficial.  And so, how I see it is that with CAD, you need to be familiar with the system to fully understand everything that's going on.  And so, sharing with stakeholders, especially, it's hard for engineers to portray exactly what's going on.  But in mixed reality, it's there.  It's just there virtually.  And they can walk around the system and have a discussion.  And, you know, it removes the need for interpretation of these CAD models or blueprints.  Thank you.  Hey, man.  Are the tools for VR development catching up or caught up to the same tools you would use for a flat screen?  Or is there still work to be done on that?  So, that's an interesting question.  If the VR tools are caught up to the same tools that we'd be using in traditional CAD modeling.  And so, we're not actually doing any modeling inside of these, you know, virtual reality systems.  Traditional CAD on computers is still superior.  You have more precise controls.  And it just, it allows them to go in greater depth on the design as well.  So, we don't do any of the design.  But the software is catching up a lot.  And already in the, you know, past couple of years that I've been working with these systems, they've matured a lot.  To where we can import the CAD into the virtual reality.  And then, if they make any updates to those systems, then it's getting to the point where we can just update it in our virtual system as well.  So, all of the interactions and things that we've developed as well can transition to the new model.  And so, I'd say that it's maturing, but it's not quite as mature as we want it to be.  Thank you.  Okay.  Well, let's go rest.  Thank you all.",
  "https://www.youtube.com/watch?v=Xu8pR8DaO0o": "{\n  \"webpage_url\": \"https://www.youtube.com/watch?v=Xu8pR8DaO0o\",\n  \"title\": \"DEF CON 31 XR VIllage - Augmented Reality and Implications on Mobile Security - Whitney Phillips\",\n  \"description\": \"A discussion of the privacy and security implications of Augmented Reality, especially in the context of Mobile Security.\",\n  \"channel_url\": \"https://www.youtube.com/channel/UC6Om9kAkl32dWlDSNlDS9Iw\",\n  \"duration\": 1675,\n  \"channel\": \"DEFCONConference\",\n  \"uploader\": \"DEFCONConference\",\n  \"upload_date\": \"20230915\"\n}\n\nThis text was transcribed using whisper model: large-v2\n\n I'm going to introduce myself.  I am speaking for the XR Village.  My name is Whitney Phillips.  I'm going to be presenting on augmented reality and the implications on mobile security.  A little bit about myself.  So I've been in InfoSec and IT for about 12 years.  I started off at Help Desk, was a system administrator after that.  Shortly after doing system administration, I became a security operations I guess analyst  that was I would do anything between running antivirus or putting encryption on PCs.  Anything just that could break my way into security.  After that, I switched over to a purple team where I did code review and I know zero coding.  So that was quite an interesting one year and I quickly got out of it.  That's where I turned to mobile application pen testing.  I started that about five years ago and have enjoyed it ever since and that's what kind  of broke me into my job here at TrustedSec right now.  So I primarily do mobile app pen testing and then I go and do gray box when they need  help for that.  So that's a little bit about me.  I was had no plan on coming to DEF CON today.  About three weeks ago, the XR Village needed an extra person to fill in, so I created my  talk and here I am and we'll give this a go.  Yay!  Woo!  All right.  So why are we here?  I am going to talk to you about augmented reality and the security and privacy issues  it has around mobile security.  So a little bit about augmented reality and I'm not going to lie, I'm more of a mobile  expert.  Augmented reality is a little bit outside of my realm, so I felt like Big Bird trying  to research something completely new and I'm like, I don't know what I'm doing.  Why am I researching something that's out of my realm?  But it's not, because we live in augmented reality every single day.  So augmented reality, it's technology that is digital, it integrates into a user's physical  environment in real time.  So anything from QR codes to Snapchat to even those fun little things that you can  put out in your house, like little 3D images, that's all augmented reality.  But what we don't really think about is it collects a wealth of our information in our  everyday life.  That's my break slide.  So when I was doing research for augmented reality, it was kind of hard.  There was a lot of different information out there.  So this is kind of the information that I've gathered.  There's two different main types of augmented reality.  There's marker-based, and that's your Snapchat, your face filtering, logos, posters, QR codes  is augmented reality.  I wouldn't even have thought of that.  It's so crazy.  Then there is without markers.  So location-based, that's GPS, projection-based, that's your 3D images that maybe you get an  app from Target and you're like, this dresser looks really cute in my house.  That's the 3D image or Pokemon Go.  And then the overlay of screen for the super in position, it's like, the best I can explain  it is like flying a drone app.  That's how I take that specific augmented reality.  So with all those, I kind of broke them down a little bit more into not so marker, markerless,  projection.  I wanted to talk about it and how I understood it.  So a lot of augmented reality, there's a lot of biometrics that are involved, especially  with face-changing ones or makeup apps.  You've got like facial recognition software, they're tracking your eyes.  Heart rate monitors is considered an augmented reality.  Those fun makeup apps where you want to put eye shadow on your face to see if it looks  really cute.  Not that everybody does that here.  Yeah, face filtering and even those clothing apps.  There's clothing apps right now that you can take a picture of yourself and see how that  fits on you.  But at the meantime, you're still scanning your whole body and you're scanning everything  about you and you're placing it in this app.  And then location-based and projection.  So I keep going on about apps that you can see how products will fit in your house.  Architecture uses augmented reality.  Navigation is augmented reality.  Manufacturing games like the Pokemon Go.  And even our military uses augmented reality for this.  One more, man?  I don't know if anybody.  I have to have those break slides.  So all right.  So here's some examples of what I found when I was doing research of what is considered  augmented reality.  Shopify has an integration into apps that will basically do the same thing as the Target  and IKEA app.  I want to see how this fits in my home.  Okay.  Well, now you're taking your GPS location, you're taking everything in your home, and  you're now giving it to that app.  The You Can Make Up cam.  Is it GIF or JIF?  World.  Roar.  You're putting dinosaurs in your house.  You're giving your information up for dinosaurs.  So other app examples.  Pokemon Go.  I know we have some Pokemon Go people in here, for sure.  I didn't take that soldier's location out.  It's supposed to go into a separate part.  But ignore that.  That's later on.  QR codes.  And again, Snapchat.  We all know we like our Snapchat filters, right?  We all know we like our Snapchat filters.  Again, I'm a sucker to it.  Don't I look really pretty as an elf?  And I have no friends.  So yeah, we have friends right here.  All right.  So with the apps, there's also devices, too.  So Apple's coming out with a new device.  Google Glass has had devices out.  Bose.  I didn't even know they have frames.  Ray-Bans have frames.  Did not know that.  Amazon.  And even our U.S. Army.  They have a tactical augmented reality.  It looks like this.  So I know the Army gets picked on a lot, and I just see something like that, and it makes  me a little uncomfortable, because I hope that maybe they're the ones sitting behind  the scenes and not doing the actual military work, but like shooting and stuff, but this  might be our future right here, and especially with these devices.  So where am I going with this?  You have these devices.  You have these apps.  Let's talk about the physical, like, let's talk about security concerns and privacy concerns  around these.  So with the physical devices, imagine you're wearing your Google Glasses and you're trying  to go into somewhere where you're not supposed to take pictures.  How are we supposed to know if you're recording or not?  I'm pretty sure plenty of people in here don't want their picture taken.  If I had glasses on right now, would you want to be in my Google thing and have your information  being sent out?  There was also earlier reports of people being assaulted for wearing those types of augmented  reality glasses, just because you're kind of drawing a fine line in people's privacy.  So that's kind of where I'm going with the physical devices, and with the apps, there's  also just general app security concerns.  So attackers can gain access to your device.  I know when you're building out an app and there's such a rush to get things out, you  have developers creating apps that are maybe not as up to par, and then, like, where are  your data stored, and then what third parties get it?  And I'm going to kind of go over those now.  So attackers gaining access, basically, they could potentially capture your images if it's  not stored properly on your device.  That's another way, if they can get a secure backup from your phone, they can pull that  information off, depending on how the app is storing it.  Ransomware on your device is another security issue.  So I don't see it as much on my end, because I'm doing more of the red teaming and mobile  pen testing, but higher profile people could be susceptible to ransomware, because they  have more information that's needed or is desired.  And then this is a little bit more in my realm, man in the middle of attacks, trying to intercept  traffic.  We don't know, and this is more where I'm coming in from the mobile perspective.  I see a lot of applications that they rush through and get the app done just to get it  out there, and then they have no concern over how their app is done, or if it's done securely.  They want to be the frontrunner of being in the augmented reality space.  Especially because it's newer, but if you look back, there still is research that is  going back several years.  I will say the information out on augmented reality reminds me of how mobile security  was five years ago when I started doing this.  So proper app development, I don't know, mobile app development lifecycle is something needed,  and I don't see that happening very quickly.  Data storage on your device, that's another privacy and security concern.  I do get a lot of apps, not augmented reality, but example, a chat app that I had stores  every photo that you take into the cache of the phone, and if you have backups enabled  on that app, well, now you have all your phone's pictures that you took in that app.  So data storage on the device, if you don't have a lock screen on your device, is an issue.  So we're lost and damaged, I mean, it's still a privacy concern.  And then these apps also store stuff up in the cloud, and we have a lot of times we do  not know, I know going and doing my gray box testing and my mobile testing, when I  first started working there, I was like, wait a minute, why is all this information heading  to all these different companies?  Why is that a thing?  Because I was so focused on data leaving the device, and you don't realize how much goes  to third parties.  But it's a huge risk, because if the third party vendor potentially gets breached, then  your data is out there, and I know that's a general concern of everybody in security.  So that's a little bit like my take on augmented reality on the app portion of it.  So we have these apps.  How are we going to maybe look at them from a technical perspective?  So I actually did a couple of these slides just a couple of days ago.  So I'm kind of going to go over something you can do as a user, and you don't even have  to have a jailbroken phone.  Just these two pieces of information.  So there's a site called APK Pure.  This is where you can pull down Android installation files, and from there, let me go over my IPA.  iPhone cake, same idea.  I did that.  I pulled my APK file down.  APK, if you're not familiar, it's the Android installation file.  IPA is the iOS installation file.  So I took those, and I stuck it in a static analysis tool called MobSF.  This is so great, if you're just wanting a high-level overview of what's going on in  your app.  So I took Snapchat, for instance.  Look at all these security issues with danger all over here.  Read your phone number, your phone state, external storage.  I mean, I guess maybe it stores something there.  Read your call logs.  Isn't that a little scary?  So and this is something I do, like, as a mobile pen tester.  This is my first step.  I'm looking at how this app is functioning.  So we've got vulnerabilities, Janice vulnerabilities, and you can install it on a really old version  of Android.  So is your data going to be secure?  No.  It also allows clear text traffic.  So yeah, it's and I was a little bit surprised.  So I thought maybe Snapchat would have had a lot of trackers to it.  It doesn't.  Just Mapbox and, you know, your general Firebase analytics.  So that was really impressive.  I was kind of hoping to see a list of trackers, but I did not.  Back to that Roar app.  I like it for some reason.  I kind of want to install it after I'm done with this talk and give them all my data.  They actually have, like, less permission settings than I would have expected.  Still the read external storage, find location, but they need that.  And that's kind of as a mobile tester.  You have to look and see and look at the app, like, why what does the app do?  Does it absolutely need to do this?  All right.  And what else did they got?  Oh, minimum version again and clear text.  So clear text allowed on this app.  But look at all these trackers.  So that's the Dinosaur app.  So you're purchasing this app to put 3D images in your house of dinosaurs, and it's tracking  all your information.  And I know I hate to pick on the military, but I like picking on them a little bit just  because our guys are using these apps, and this is a civilian version, so I shouldn't  get in trouble for this.  But our boots on the ground are using these apps, and it's just as we want to make sure  that they're secure.  But then it still needs a lot of permissions to have that app run.  And still clear text.  And another vulnerability.  But why do we have servers over in different countries if it's a U.S. app?  So I know.  All this, I just wanted to kind of show from a mobile pen tester's perspective how I would  securely look at these devices.  I'm going to go in just a little bit more on my mobile.  Say you want to take it a step further.  You need a jailbroken or rooted device.  I use Magisk personally for my Android.  I use Uncover for my iOS.  And then these are my steps that I would take to look at local data storage.  I would have my rooted or jailbroken phone.  And then if I was wanting to not get the app from the actual app store, I would just create  a fake account and do it that way.  But if you're just going to do static analysis and you don't need a phone, APKPure and MobSF  is the best way to go.  So this is a little bit more technical part of my talk.  I use ADB for Android, and that is my toolkit that I use a lot to do more internal investigation  of the device and the application.  Burp Suite I use to do that traffic to see what the application, how it functions, which  you know, what information is being sent.  Frida is another app that I use.  This app, if you have to, like, be in a wrong state of mind to learn how this app functions.  So I use Freedump a lot to dump the memory of a device.  So sometimes applications will dump information in runtime.  This is a great tool.  So if they don't reboot their phone and you're looking for maybe potential security flaws  or application issues, I would use this tool right here.  This one I have just left over, you pull, like, your unencrypted IPA file and that's  another way you can statically analysis an IPA for iOS, and that's Freedump.  Tied to Frida, let me take a break here.  Tied to Frida is Objection, and that's another tool that I use in and out, and if I were  to be taking these augmented reality apps, I would go through and kind of inspect them  a little bit deeper.  So Objection, you can do file system exploration, you can bypass certificate pinning, dump key  chains, so if they're not storing things or if they're storing stuff in the key chain  properly or maybe they're not storing it properly, you can get that information.  Memory leak issues, you can manipulate objects.  This is a starting command for Objection, you didn't think this was going to be a mobile  talk now, did you?  It is.  So these are the kind of commands that you can run with Objection, basically these are  my go-to when I'm, like, searching through an app, if it's got SSL, like a cert pinning  so I can't use burp suite, I'll use the bypass, if it detects root detection, I will bypass  that with the root disable, key store, looking for classes, I like to take the classes and  try to launch them in the app without being logged in to see if I can get some sort of  different information, and that's not so much with these apps that I was talking about before,  but a lot of times, like maybe a banking app, can I launch that activity, will it drop me  in to the banking software?  So iOS, NS user defaults, sometimes a lot of information gets stored in that, and you'd  be surprised, yeah, you'd be surprised, I've seen passwords in there, so it's definitely  a good thing to look at.  Credentials, I have the defaults twice, I never fix that.  Whoops.  Keychain dump, and then cookies, again, we need to see if the app is storing things  properly in the keychain.  So I know I kind of touched on, whoopsie, a little crazy on the slides, I know I kind of  touched on iOS a little bit, but I use JEDX too for decompiling my Android apps, that's a  good app to have a nice, if you're a gooey person like I am, you can take that APK, you  don't even have to do anything with it, you stick it in the app and it shows you all your  classes, and it's an easier way to be able to look at that information and see what the  app has.  So you have your mobile apps, what am I looking for, secrets, keys, credentials, URLs, IP  addresses, email addresses, and then again, with the augmented reality, if I was given  that as an assignment, I would probably look to see what kind of URLs that aren't tied  to that company, why are they going to that, is it necessary, because there is a lot of  information that gets stored in apps that you don't normally think you would want to see.  So after all this, circling back around with our augmented reality and our mobile apps,  what should we do as humans to keep ourselves secure? So avoiding giving out too much  information, a lot of times it's easy to just put your name down and just let it auto  fill in the app and then hope for the best. So give half information, if it's an app that  does not need your information, you do not need to give that app your information. So  have a Jane Doe or whatever name and then Paul won't be able to user enumerate you.  So do you want to come finish the rest of your talk? No. Anyways, VPN, that's another one.  I've been told I should use it. I probably will eventually. And then going back to the  physical devices, so firmware. Firmware is huge when you have your physical devices, like  your glasses, keeping those up to date. I know when I worked in my previous job, I did a lot  of drone stuff and keeping firmware up to date on your drones is a necessary evil. So  I know it can be a pain in the butt and it probably sometimes might make your drone act  funny. I may or may not have had that happen in my life before. Maybe it will just fly up in  your house for no reason. But firmware updates are needed. And then locking your device, I  know it's something really simple to say, but I feel like people still don't do that. And  two-factor authentication is I feel like the best way to make sure that your information  doesn't get put out there. And like I said, to push that first bullet point is don't put your  information out there if it doesn't need to be. That's like the best information that I could  give for keeping yourself secure in these kind of apps. Because they are going to start  pushing more tracking information and they are going to start getting more of our data and  sending it out so then they can use it for other things. You know, like when you think about  buying a pair of shoes and it shows up in your ads, right? So I might have went too fast.  But some training material that I often like to suggest. Shameless post of my job. The blog. Or  you can hear me on a podcast. But when I first started doing mobile security, like this was my  bread and butter. The OWASP mobile security testing guide. It's my thing. Like I liked it. It  has good information. And there's also a website hack tricks that I've referenced quite often  now. But I did not have that on that slide. So with that, I think I have 30 minutes. That's a  solid 30 minutes. If anybody has any questions, we can answer them. We. There's a royal we. I  can answer them down there. And here's my information if you want to just message me, that's  fine, too. But yeah. That's my first DEF CON talk. Woo!"
}