**Rethinking Detection Engineering: Threat Scoring for Prioritization**

- Current defensive tools often generate an overwhelming number of alerts, many of which are dismissed without proper justification. 
- Adding context to alert event data helps analysts prioritize and triage alerts effectively. 
- This talk proposes a method for prioritizing composite event fields and defining a score to programmatically address alerts. 

**Speaker Introduction**

- Name: Josh Krueger 
- Role: Threat hunter at SpecterOps, focusing on program development for security operating centers. 
- Hobbies: Astrophotography and spending time with family. 

**Pain Points in Alert Prioritization**

- Lack of Structure: Analysts may choose alerts subjectively, leading to potential missed threats. 
- Prioritization Based on Assumptions: Scoring structures may exist but are stagnant and untested, relying on factors that are assumed to be malicious without validation. 
- Singular Factor Determination: Using only one factor for prioritization can work for precise alerts but may rank multiple alerts as high for complex technique-based alerts. 
- Stagnant Scoring Structures: Scoring structures may exist but are not updated to account for new research, environment changes, or personnel/process variations. 

**Defining Key Terms**

- Triage: Adding context to a base condition to establish priority, following the "funnel of fidelity" concept by Jared Atkinson. 
- Subjective vs. Objective Metrics: Subjective metrics are opinion-based (e.g., a rating scale), while objective metrics are fact-based (e.g., number of tickets closed). 

**Understanding "Priority"**

- "Priority" stems from "a priori" (derived from logic/reason) and "a posteriori" (proven/observed fact). 
- Both methods are used in prioritization but can lead to misconceptions if not properly validated. 

**Establishing a Base Condition**

- Effective prioritization of malicious events requires deriving a base condition and collecting relevant telemetry data. 
- Example: Service creation, including both malicious and legitimate instances. 
- Consideration: Choosing the appropriate collection method (e.g., event ID vs. raw registry key creation event). 

**Adding Contextual Factors**

- The base condition provides limited information for triage decisions. 
- Contextual factors are needed to determine if an event is benign or malicious. 
- Developing logical hypotheses based on research is essential for identifying relevant factors. 

**Hypotheses and Assumptions**

- Assumption 1: Remote service creation is a malicious indicator (based on MITRE ATT&CK tactics). 
- Assumption 2: "Services.exe" should be the binary creating the registry key in normal situations (based on underlying technology research). 
- Assumption 3: Auto-start type services are a malicious indicator (assuming adversaries want payload execution upon startup). 
- Assumption 4: "Sc.exe" should be the process requesting service creation in normal situations (deviation from baseline behavior). 

**Testing Hypotheses with Data Analytics**

- Validation and testing are necessary to turn hypotheses into empirical evidence. 
- Consideration: Ensure a diverse and representative data pool, accounting for variations across environments. 

**Descriptive Statistics Techniques**

- Measures of Frequency: Identify prevalence and deviation in the environment. 
- Measures of Tendency/Prevalence of Execution: Determine common responses or events. 
- Measures of Variation: Measure the range of data (high to low). 
- Measures of Position: Determine scores' variations between multiple data sets within a data pool. 

**Applying Descriptive Statistics**

- Assumption 1 (Remote Service Creation): Measures of frequency show that legitimate remote service creation is rare across three environments. 
- Scoring: Remote service creation events are scored higher than local creations. 

**Inferential Statistics Techniques**

- Bayes' Theorem: Determines probability based on prior collected data. 
- Analysis of Variance (ANOVA): Examines differences in probability of variations. 
- Regression: Predicts outcome scores in new data sets. 

**Applying Inferential Statistics**

- Assumption 2 ("Services.exe" Binary): Measures of frequency show that "services.exe" accounts for 95% of occurrences, while "registry.exe" accounts for 5%. 
- Scoring: Assign a higher malicious score to "registry.exe" due to its rarity. 

**Combining Descriptive and Inferential Statistics**

- Assumption 3 (Auto-Start Services): Utilize Bayes' theorem to determine the range of probability without considering maliciousness. 
- Identify the marginal probability: 25% chance of a service being auto-start based on historical data. 
- Infer probability distribution and determine a scoring range. 

**Measures of Position**

- Assumption 4 ("Sc.exe" Process): Identify variations between multiple data sets or snapshots over time. 
- Calculate five key values: minimum, first quartile, median, third quartile, and maximum. 
- Compute standard deviation to identify marginal probability and flexible weight for scoring. 

**Developing an Empirical Score**

- Statistical analysis identifies behavioral relationships that inform scoring. 
- Empirical scoring must be documented for future research, tuning, and accounting for variations. 
- Machine learning could aid in validating metrics and identifying changes in conclusions. 

**Addressing Challenges**

- Stagnating Conclusions: Constant testing is required to validate hypotheses and update conclusions. 
- Environmental, Endpoint Software, and Process Changes: Adapt scoring to these changes over time. 
- Margin of Error: Acknowledge and account for incomplete knowledge and bias. 

**References**

- Funnel of Fidelity by Jared Atkinson 
- Johnny Johnson's automated detection pipeline Jupyter Notebook project on GitHub 
- Research by the speaker and Johnny Johnson 
- Statistical analysis resources (links provided) 

**Conclusion**

- The speaker summarizes the key points and invites questions.